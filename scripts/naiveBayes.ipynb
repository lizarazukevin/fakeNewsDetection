{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 0 out of 210007\n",
      "Training 1 out of 210007\n",
      "Training 2 out of 210007\n",
      "Training 3 out of 210007\n",
      "Training 4 out of 210007\n",
      "Training 5 out of 210007\n",
      "Training 6 out of 210007\n",
      "Training 7 out of 210007\n",
      "Training 8 out of 210007\n",
      "Training 9 out of 210007\n",
      "Training 10 out of 210007\n",
      "Training 11 out of 210007\n",
      "Training 12 out of 210007\n",
      "Training 13 out of 210007\n",
      "Training 14 out of 210007\n",
      "Training 15 out of 210007\n",
      "Training 16 out of 210007\n",
      "Training 17 out of 210007\n",
      "Training 18 out of 210007\n",
      "Training 19 out of 210007\n",
      "Training 20 out of 210007\n",
      "Done Training!\n",
      "Matrix 0 out of 5000\n",
      "Matrix 1 out of 5000\n",
      "Matrix 2 out of 5000\n",
      "Matrix 3 out of 5000\n",
      "Matrix 4 out of 5000\n",
      "Matrix 5 out of 5000\n",
      "Matrix 6 out of 5000\n",
      "Matrix 7 out of 5000\n",
      "Matrix 8 out of 5000\n",
      "Matrix 9 out of 5000\n",
      "Evaluating 0 out of 5000\n",
      "Evaluating 1 out of 5000\n",
      "Evaluating 2 out of 5000\n",
      "Evaluating 3 out of 5000\n",
      "Evaluating 4 out of 5000\n",
      "Evaluating 5 out of 5000\n",
      "Evaluating 6 out of 5000\n",
      "Evaluating 7 out of 5000\n",
      "Evaluating 8 out of 5000\n",
      "Evaluating 9 out of 5000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "('Lengths must match to compare', (5000,), (10,))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 117\u001b[0m\n\u001b[0;32m    114\u001b[0m model \u001b[39m=\u001b[39m train(train_df)\n\u001b[0;32m    116\u001b[0m \u001b[39m# Evaluate the Naive Bayes classifier on the testing dataset\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m accuracy \u001b[39m=\u001b[39m evaluate(test_df, model)\n\u001b[0;32m    119\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mAccuracy: \u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(accuracy \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m))\n",
      "Cell \u001b[1;32mIn[14], line 105\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(df, model)\u001b[0m\n\u001b[0;32m    102\u001b[0m         y_pred\u001b[39m.\u001b[39mappend(\u001b[39m0\u001b[39m)\n\u001b[0;32m    104\u001b[0m \u001b[39m# Calculate accuracy\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m accuracy \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(y_pred \u001b[39m==\u001b[39;49m y_true) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(df)\n\u001b[0;32m    107\u001b[0m \u001b[39mreturn\u001b[39;00m accuracy\n",
      "File \u001b[1;32mc:\\Users\\kliza\\anaconda3\\envs\\fakeNews\\Lib\\site-packages\\pandas\\core\\ops\\common.py:81\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[0;32m     79\u001b[0m other \u001b[39m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 81\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, other)\n",
      "File \u001b[1;32mc:\\Users\\kliza\\anaconda3\\envs\\fakeNews\\Lib\\site-packages\\pandas\\core\\arraylike.py:40\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[39m@unpack_zerodim_and_defer\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m__eq__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__eq__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[1;32m---> 40\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cmp_method(other, operator\u001b[39m.\u001b[39;49meq)\n",
      "File \u001b[1;32mc:\\Users\\kliza\\anaconda3\\envs\\fakeNews\\Lib\\site-packages\\pandas\\core\\series.py:6097\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   6094\u001b[0m rvalues \u001b[39m=\u001b[39m extract_array(other, extract_numpy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, extract_range\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   6096\u001b[0m \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(\u001b[39mall\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 6097\u001b[0m     res_values \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mcomparison_op(lvalues, rvalues, op)\n\u001b[0;32m   6099\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_construct_result(res_values, name\u001b[39m=\u001b[39mres_name)\n",
      "File \u001b[1;32mc:\\Users\\kliza\\anaconda3\\envs\\fakeNews\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:263\u001b[0m, in \u001b[0;36mcomparison_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(rvalues, (np\u001b[39m.\u001b[39mndarray, ABCExtensionArray)):\n\u001b[0;32m    259\u001b[0m     \u001b[39m# TODO: make this treatment consistent across ops and classes.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m#  We are not catching all listlikes here (e.g. frozenset, tuple)\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39m#  The ambiguous case is object-dtype.  See GH#27803\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lvalues) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(rvalues):\n\u001b[1;32m--> 263\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    264\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mLengths must match to compare\u001b[39m\u001b[39m\"\u001b[39m, lvalues\u001b[39m.\u001b[39mshape, rvalues\u001b[39m.\u001b[39mshape\n\u001b[0;32m    265\u001b[0m         )\n\u001b[0;32m    267\u001b[0m \u001b[39mif\u001b[39;00m should_extension_dispatch(lvalues, rvalues) \u001b[39mor\u001b[39;00m (\n\u001b[0;32m    268\u001b[0m     (\u001b[39misinstance\u001b[39m(rvalues, (Timedelta, BaseOffset, Timestamp)) \u001b[39mor\u001b[39;00m right \u001b[39mis\u001b[39;00m NaT)\n\u001b[0;32m    269\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_object_dtype(lvalues\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m    270\u001b[0m ):\n\u001b[0;32m    271\u001b[0m     \u001b[39m# Call the method on lvalues\u001b[39;00m\n\u001b[0;32m    272\u001b[0m     res_values \u001b[39m=\u001b[39m op(lvalues, rvalues)\n",
      "\u001b[1;31mValueError\u001b[0m: ('Lengths must match to compare', (5000,), (10,))"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub('[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def train(df):\n",
    "    # Preprocess the text data\n",
    "    df['title'] = df['title'].apply(preprocess)\n",
    "    df['text'] = df['text'].apply(preprocess)\n",
    "    df['subject'] = df['subject'].apply(preprocess)\n",
    "\n",
    "    # Create vocabulary\n",
    "    vocabulary = set()\n",
    "    for text in df['title']:\n",
    "        vocabulary.update(text.split())\n",
    "    for text in df['text']:\n",
    "        vocabulary.update(text.split())\n",
    "\n",
    "    # Calculate prior probabilities\n",
    "    num_real = (df['label'] == 'real').sum()\n",
    "    num_fake = (df['label'] == 'fake').sum()\n",
    "    total = num_real + num_fake\n",
    "    prior_real = num_real / total\n",
    "    prior_fake = num_fake / total\n",
    "\n",
    "    # Calculate conditional probabilities\n",
    "    count_real = np.zeros(len(vocabulary))\n",
    "    count_fake = np.zeros(len(vocabulary))\n",
    "    for i, word in enumerate(vocabulary):\n",
    "\n",
    "        # if i == int(len(vocabulary) / 10000):\n",
    "        #     break\n",
    "\n",
    "        print(f\"Training {i} out of {len(vocabulary)}\")\n",
    "\n",
    "        for text, label in zip(df['text'], df['label']):\n",
    "            if word in text.split():\n",
    "                if label == 'real':\n",
    "                    count_real[i] += 1\n",
    "                else:\n",
    "                    count_fake[i] += 1\n",
    "\n",
    "    conditional_real = (count_real + 1) / (num_real + 2)\n",
    "    conditional_fake = (count_fake + 1) / (num_fake + 2)\n",
    "\n",
    "    # Store model parameters\n",
    "    model = {\n",
    "        'vocabulary': sorted(vocabulary),\n",
    "        'prior_real': prior_real,\n",
    "        'prior_fake': prior_fake,\n",
    "        'conditional_real': conditional_real,\n",
    "        'conditional_fake': conditional_fake\n",
    "    }\n",
    "\n",
    "    print(\"Done Training!\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate(df, model):\n",
    "    # Preprocess the text data\n",
    "    df['title'] = df['title'].apply(preprocess)\n",
    "    df['text'] = df['text'].apply(preprocess)\n",
    "    df['subject'] = df['subject'].apply(preprocess)\n",
    "\n",
    "    # Create document-term matrix\n",
    "    X = np.zeros((len(df), len(model['vocabulary'])))\n",
    "    for i, text in enumerate(df['text']):\n",
    "        \n",
    "        # if i == 10:\n",
    "        #     break\n",
    "        print(f\"Matrix {i} out of {len(df['text'])}\")\n",
    "\n",
    "        for j, word in enumerate(model['vocabulary']):\n",
    "            X[i,j] = text.split().count(word)\n",
    "\n",
    "    # Make predictions\n",
    "    y_true = (df['label'] == 'real').astype(int)\n",
    "    y_pred = []\n",
    "    for i in range(len(df)):\n",
    "\n",
    "        # if i == 10:\n",
    "        #     break\n",
    "        print(f\"Evaluating {i} out of {len(df)}\")\n",
    "\n",
    "        log_prob_real = np.log(model['prior_real'])\n",
    "        log_prob_fake = np.log(model['prior_fake'])\n",
    "        for j in range(len(model['vocabulary'])):\n",
    "            if X[i,j] > 0:\n",
    "                log_prob_real += X[i,j] * np.log(model['conditional_real'][j])\n",
    "                log_prob_fake += X[i,j] * np.log(model['conditional_fake'][j])\n",
    "        if log_prob_real > log_prob_fake:\n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(0)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = sum(y_pred == y_true) / len(df)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv('../datasets/test1.csv')\n",
    "test_df = pd.read_csv('../datasets/test2.csv')\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "model = train(train_df)\n",
    "\n",
    "# Evaluate the Naive Bayes classifier on the testing dataset\n",
    "accuracy = evaluate(test_df, model)\n",
    "\n",
    "print('Accuracy: {:.2f}%'.format(accuracy * 100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fakeNews",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
